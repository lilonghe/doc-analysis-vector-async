from celery import Celery
import os
import json
from datetime import datetime
import time
import traceback
from error_handler import (
    retry_with_backoff, 
    log_and_handle_error, 
    error_tracker,
    ErrorHandler,
    RetryableError,
    NonRetryableError
)
from dotenv import load_dotenv
from chroma_db import ChromaVectorDB

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# å…¨å±€ChromaDBå®ä¾‹ï¼ˆå¯¹äºCelery workerï¼‰
_vector_db = None

def get_vector_db():
    """è·å–ChromaDBå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _vector_db
    if _vector_db is None:
        print("ğŸ”— åˆå§‹åŒ–ChromaDBè¿æ¥...")
        _vector_db = ChromaVectorDB()
    return _vector_db

# è·å–Redis URLï¼Œæ”¯æŒç¯å¢ƒå˜é‡é…ç½®
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379/0')

# åˆ›å»ºCeleryå®ä¾‹
celery_app = Celery(
    'doc_processor',
    broker=REDIS_URL,
    backend=REDIS_URL
)

# Celeryé…ç½® - å¢å¼ºå¯é æ€§
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    worker_concurrency=3,  # å¹¶å‘å¤„ç†3ä¸ªä»»åŠ¡
    
    # ä»»åŠ¡å¯é æ€§é…ç½®
    task_acks_late=True,  # ä»»åŠ¡å®Œæˆåæ‰ç¡®è®¤ï¼Œé¿å…ä»»åŠ¡ä¸¢å¤±
    worker_prefetch_multiplier=1,  # æ¯ä¸ªworkerä¸€æ¬¡åªå¤„ç†ä¸€ä¸ªä»»åŠ¡
    task_reject_on_worker_lost=True,  # workerä¸¢å¤±æ—¶æ‹’ç»ä»»åŠ¡
    
    # ç»“æœæŒä¹…åŒ–é…ç½®
    result_expires=3600,  # ç»“æœä¿å­˜1å°æ—¶
    result_persistent=True,  # ç»“æœæŒä¹…åŒ–
    
    # é‡è¯•é…ç½®
    task_default_retry_delay=60,  # é»˜è®¤é‡è¯•å»¶è¿Ÿ60ç§’
    task_max_retries=3,  # æœ€å¤§é‡è¯•æ¬¡æ•°
    
    # Redisè¿æ¥é…ç½®
    broker_connection_retry=True,
    broker_connection_retry_on_startup=True,
    broker_connection_max_retries=10,
)

def update_file_status(file_id: str, status: str, progress: int, message: str):
    """æ›´æ–°æ–‡ä»¶å¤„ç†çŠ¶æ€"""
    try:
        from database import get_database_manager
        db_manager = get_database_manager()
        
        success = db_manager.update_file_status(file_id, status, progress, message)
        if not success:
            print(f"Warning: Failed to update status for file {file_id}")
            
        # è®°å½•å¤„ç†é˜¶æ®µæ—¥å¿—
        if status in ["parsing", "chunking", "embedding", "storing"]:
            db_manager.log_processing_stage(file_id, status, "started", message)
        elif status == "completed":
            db_manager.log_processing_stage(file_id, "processing", "completed", message)
        elif status == "error":
            db_manager.log_processing_stage(file_id, "processing", "failed", message)
            
    except Exception as e:
        print(f"Error updating file status: {e}")
        import traceback
        traceback.print_exc()

@celery_app.task(bind=True)
def process_document(self, file_id: str):
    """å¤„ç†å•ä¸ªæ–‡æ¡£çš„ä¸»ä»»åŠ¡ï¼ˆå¸¦é”™è¯¯å¤„ç†å’Œé‡è¯•ï¼‰"""
    try:
        from database import get_database_manager
        db_manager = get_database_manager()
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ­¤ä»»åŠ¡
        if error_tracker.should_skip_task(file_id, max_errors=3):
            error_msg = f"âŒ ä»»åŠ¡è·³è¿‡: é”™è¯¯æ¬¡æ•°è¿‡å¤š ({error_tracker.get_error_count(file_id)} æ¬¡)"
            update_file_status(file_id, "error", 0, error_msg)
            return {"file_id": file_id, "status": "skipped", "reason": "too_many_errors"}
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_record = db_manager.get_file_record(file_id)
        if not file_record:
            raise NonRetryableError("æ–‡ä»¶ä¿¡æ¯ä¸å­˜åœ¨", "file_error")
        
        filepath = file_record.filepath
        filename = file_record.filename
        
        print(f"ğŸš€ [CELERY] å¼€å§‹å¤„ç†æ–‡æ¡£ä»»åŠ¡: {file_id}")
        print(f"ğŸ“„ [CELERY] æ–‡ä»¶ä¿¡æ¯: {filename} ({filepath})")
        
        # è®°å½•å¼€å§‹æ—¶é—´
        import time
        start_time = time.time()
        
        # é˜¶æ®µ1: MinerUè§£ææ–‡æ¡£ï¼ˆå¸¦é‡è¯•ï¼‰
        print(f"ğŸ” [CELERY] é˜¶æ®µ1: å¼€å§‹MinerUè§£æ...")
        update_file_status(file_id, "parsing", 10, "MinerUè§£æä¸­...")
        parsing_start = time.time()
        extracted_content = parse_document_with_retry(file_id, filepath)
        parsing_duration = time.time() - parsing_start
        print(f"âœ… [CELERY] é˜¶æ®µ1å®Œæˆ: è§£æè€—æ—¶ {parsing_duration:.2f}s")
        db_manager.log_processing_stage(file_id, "parsing", "completed", "æ–‡æ¡£è§£æå®Œæˆ", parsing_duration)
        
        # æ›´æ–°æ–‡æ¡£é¡µæ•°
        if extracted_content.get("metadata", {}).get("total_pages"):
            db_manager.update_file_results(file_id, total_pages=extracted_content["metadata"]["total_pages"])
        
        update_file_status(file_id, "parsing", 30, "æ–‡æ¡£è§£æå®Œæˆ")
        
        # é˜¶æ®µ2: æ™ºèƒ½åˆ†å—ï¼ˆå¸¦é‡è¯•ï¼‰
        print(f"âœ‚ï¸ [CELERY] é˜¶æ®µ2: å¼€å§‹æ™ºèƒ½åˆ†å—...")
        update_file_status(file_id, "chunking", 40, "æ™ºèƒ½åˆ†å—ä¸­...")
        chunking_start = time.time()
        chunks = chunk_document_with_retry(file_id, extracted_content)
        chunking_duration = time.time() - chunking_start
        print(f"âœ… [CELERY] é˜¶æ®µ2å®Œæˆ: åˆ†å—è€—æ—¶ {chunking_duration:.2f}sï¼Œå…±ç”Ÿæˆ {len(chunks)} å—")
        db_manager.log_processing_stage(file_id, "chunking", "completed", f"åˆ†å—å®Œæˆï¼Œå…±{len(chunks)}å—", chunking_duration)
        
        # æ›´æ–°å—æ•°é‡
        db_manager.update_file_results(file_id, chunks_count=len(chunks))
        
        update_file_status(file_id, "chunking", 60, f"åˆ†å—å®Œæˆï¼Œå…±{len(chunks)}å—")
        print(f"ğŸ“Š [CELERY] åˆ†å—ç»Ÿè®¡: {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        # é˜¶æ®µ3: å‘é‡åŒ–ï¼ˆå¸¦é‡è¯•ï¼‰
        print(f"ğŸ§® [CELERY] é˜¶æ®µ3: å¼€å§‹å‘é‡åŒ–...")
        update_file_status(file_id, "embedding", 70, "å‘é‡åŒ–ä¸­...")
        embedding_start = time.time()
        embeddings = generate_embeddings_with_retry(file_id, chunks)
        embedding_duration = time.time() - embedding_start
        print(f"âœ… [CELERY] é˜¶æ®µ3å®Œæˆ: å‘é‡åŒ–è€—æ—¶ {embedding_duration:.2f}sï¼Œå…±{len(embeddings)}ä¸ªå‘é‡")
        db_manager.log_processing_stage(file_id, "embedding", "completed", "å‘é‡åŒ–å®Œæˆ", embedding_duration)
        
        update_file_status(file_id, "embedding", 90, "å‘é‡åŒ–å®Œæˆ")
        
        # é˜¶æ®µ4: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆå¸¦é‡è¯•ï¼‰
        print(f"ğŸ’¾ [CELERY] é˜¶æ®µ4: å¼€å§‹å­˜å‚¨å‘é‡...")
        update_file_status(file_id, "storing", 95, "å­˜å‚¨å‘é‡ä¸­...")
        storing_start = time.time()
        store_with_retry(file_id, chunks, embeddings)
        storing_duration = time.time() - storing_start
        print(f"âœ… [CELERY] é˜¶æ®µ4å®Œæˆ: å­˜å‚¨è€—æ—¶ {storing_duration:.2f}s")
        db_manager.log_processing_stage(file_id, "storing", "completed", "å‘é‡å­˜å‚¨å®Œæˆ", storing_duration)
        
        # å®Œæˆ
        total_duration = time.time() - start_time
        completion_message = f"âœ… å¤„ç†å®Œæˆ (è€—æ—¶: {total_duration:.1f}ç§’)"
        update_file_status(file_id, "completed", 100, completion_message)
        
        print(f"ğŸ‰ [CELERY] ä»»åŠ¡å®Œæˆ: {filename} å¤„ç†æˆåŠŸï¼Œæ€»è€—æ—¶ {total_duration:.2f}s")
        print(f"ğŸ“ˆ [CELERY] å¤„ç†ç»Ÿè®¡: {len(chunks)} å—ï¼Œ{total_duration:.1f}s å®Œæˆ")
        
        return {
            "file_id": file_id,
            "filename": filename,
            "chunks_count": len(chunks),
            "total_duration": total_duration,
            "status": "completed"
        }
        
    except NonRetryableError as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥ (ä¸å¯é‡è¯•): {str(e)}"
        update_file_status(file_id, "error", 0, error_msg)
        print(f"æ–‡ä»¶å¤„ç†å¤±è´¥ (ä¸å¯é‡è¯•): {file_id}, é”™è¯¯: {str(e)}")
        raise
        
    except Exception as e:
        # è®°å½•å’Œå¤„ç†é”™è¯¯
        handled_error = log_and_handle_error(file_id, e, "document_processing")
        
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(handled_error)}"
        update_file_status(file_id, "error", 0, error_msg)
        print(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {file_id}, é”™è¯¯: {str(e)}")
        print(traceback.format_exc())
        
        # å¦‚æœæ˜¯å¯é‡è¯•é”™è¯¯ï¼Œé‡æ–°æŠ›å‡ºè®©Celeryé‡è¯•
        if isinstance(handled_error, RetryableError):
            self.retry(countdown=60, max_retries=2)
        
        raise

@retry_with_backoff(max_retries=2, base_delay=1.0)
def parse_document_with_retry(file_id: str, filepath: str) -> dict:
    """å¸¦é‡è¯•çš„æ–‡æ¡£è§£æ"""
    try:
        return mineru_parse_document(filepath)
    except Exception as e:
        handled_error = log_and_handle_error(file_id, e, "parsing")
        raise handled_error

@retry_with_backoff(max_retries=2, base_delay=0.5)
def chunk_document_with_retry(file_id: str, parsed_content: dict) -> list:
    """å¸¦é‡è¯•çš„æ–‡æ¡£åˆ†å—"""
    try:
        return intelligent_chunking(parsed_content)
    except Exception as e:
        handled_error = log_and_handle_error(file_id, e, "chunking")
        raise handled_error

@retry_with_backoff(max_retries=3, base_delay=2.0)
def generate_embeddings_with_retry(file_id: str, chunks: list) -> list:
    """å¸¦é‡è¯•çš„å‘é‡ç”Ÿæˆ"""
    try:
        return generate_embeddings(chunks)
    except Exception as e:
        handled_error = log_and_handle_error(file_id, e, "embedding")
        raise handled_error

@retry_with_backoff(max_retries=2, base_delay=1.0)
def store_with_retry(file_id: str, chunks: list, embeddings: list):
    """å¸¦é‡è¯•çš„æ•°æ®åº“å­˜å‚¨"""
    try:
        return store_to_vector_db(file_id, chunks, embeddings)
    except Exception as e:
        handled_error = log_and_handle_error(file_id, e, "storing")
        raise handled_error

def mineru_parse_document(filepath: str) -> dict:
    """ä½¿ç”¨MinerUè§£ææ–‡æ¡£"""
    from mineru_parser import MinerUParser
    
    print(f"MinerUè§£ææ–‡æ¡£: {filepath}")
    parser = MinerUParser()
    
    try:
        result = parser.parse_pdf(filepath)
        print(f"è§£æå®Œæˆ: é¡µæ•°={result['metadata']['total_pages']}, è¡¨æ ¼={result['metadata']['tables_count']}")
        return result
    except Exception as e:
        print(f"MinerUè§£æå¤±è´¥: {str(e)}")
        raise

def intelligent_chunking(parsed_content: dict) -> list:
    """ä½¿ç”¨Ollamaè¿›è¡Œæ™ºèƒ½åˆ†å—"""
    try:
        from ollama_processor import OllamaProcessor
        
        print("ä½¿ç”¨Ollamaè¿›è¡Œæ™ºèƒ½åˆ†å—...")
        processor = OllamaProcessor()
        chunks = processor.intelligent_chunk_document(parsed_content)
        
        print(f"æ™ºèƒ½åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} å—")
        return chunks
        
    except Exception as e:
        print(f"Ollamaæ™ºèƒ½åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ: {str(e)}")
        return fallback_chunking(parsed_content)

def fallback_chunking(parsed_content: dict) -> list:
    """å¤‡ç”¨åˆ†å—æ–¹æ¡ˆ"""
    content = parsed_content.get("content", "")
    chunks = []
    
    if not content.strip():
        return chunks
    
    # ç®€å•æŒ‰é•¿åº¦åˆ†å—
    chunk_size = 1000
    for i in range(0, len(content), chunk_size):
        chunk_content = content[i:i + chunk_size]
        if chunk_content.strip():
            chunks.append({
                "title": f"æ–‡æ¡£ç‰‡æ®µ {len(chunks) + 1}",
                "content": chunk_content.strip(),
                "summary": f"æ–‡æ¡£çš„ç¬¬{len(chunks) + 1}ä¸ªç‰‡æ®µ",
                "type": "fallback_chunk"
            })
    
    return chunks

def generate_embeddings(chunks: list) -> list:
    """ç”Ÿæˆå‘é‡åµŒå…¥"""
    try:
        from ollama_processor import OllamaProcessor
        
        print("ä½¿ç”¨Ollamaç”Ÿæˆå‘é‡åµŒå…¥...")
        processor = OllamaProcessor()
        embeddings = processor.generate_embeddings(chunks)
        
        print(f"å‘é‡åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
        return embeddings
        
    except Exception as e:
        print(f"Ollamaå‘é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨éšæœºå‘é‡: {str(e)}")
        return fallback_embeddings(chunks)

def fallback_embeddings(chunks: list) -> list:
    """å¤‡ç”¨å‘é‡ç”Ÿæˆæ–¹æ¡ˆ"""
    import random
    embeddings = []
    for chunk in chunks:
        # ç”Ÿæˆéšæœºå‘é‡ï¼ˆå®é™…åº”ç”¨ä¸­ä¸å»ºè®®ï¼‰
        embedding = [random.random() for _ in range(768)]  # Ollama nomic-embed-textç»´åº¦
        embeddings.append(embedding)
    return embeddings

def store_to_vector_db(file_id: str, chunks: list, embeddings: list):
    """å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
    try:
        from database import get_database_manager
        
        print("å­˜å‚¨åˆ°ChromaDBå‘é‡æ•°æ®åº“...")
        db = get_vector_db()
        db_manager = get_database_manager()
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_record = db_manager.get_file_record(file_id)
        if not file_record:
            raise Exception("æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯")
            
        filename = file_record.filename
        
        # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        success = db.store_document_chunks(
            file_id=file_id,
            filename=filename,
            chunks=chunks,
            embeddings=embeddings
        )
        
        if success:
            print(f"âœ… å·²å°† {len(chunks)} ä¸ªæ–‡æ¡£å—å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“")
        else:
            raise Exception("å‘é‡æ•°æ®åº“å­˜å‚¨å¤±è´¥")
            
    except Exception as e:
        print(f"âŒ å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
        raise

if __name__ == '__main__':
    # å¯åŠ¨Celery worker
    celery_app.start()